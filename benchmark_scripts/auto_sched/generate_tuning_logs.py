from time import time
from pathlib import Path

import tvm
from tvm import auto_scheduler
import tvm.relay.testing

# Local modules
import configs
from get_network import get_relay_network
"""
This script takes the relay model generated by get_network.py,
tunes the model with Ansor, and stores the tuning records to
the path designated in configs.py. In addition, it also makes
a copy of the most recent tuning record for a model with
the parameters set in configs.py

Continuous tuning (i.e. continuing from an existing tuning session)
is supported.
"""


def auto_scheduler_tuning(tasks, task_weights, tuning_record_path: str,
                          previous_tuning_log: str):
    """Tune the tasks using Ansor/Auto_scheduler
    and stores the tuning results as tuning logs to tuning_record_path

    Parameters
    ----------
    task: List[SearchTask]
        A list of search tasks, generated by :func:`~tvm.auto_scheduler.extract_tasks~`.
        In general, each of these search tasks should roughly correspond to one
        tensor operator.
    task_weights: List[int]
        The weights of each search task (the number of appearances of each operator in
        the whole model). This is generated by :func:`~tvm.auto_scheduler.extract_tasks~`
    tuning_record_path: str | Path
        The path in which the tuning records will be stored
    previous_tuning_log: str | Path | None
        (Optional, must be supplied if you want continuous tuning to work) The path to
        the previous tuning log, which the current tuning session will make reference to
        if continuous tuning is enabled in configs.py
    """
    measure_ctx = auto_scheduler.LocalRPCMeasureContext(repeat=3,
                                                        min_repeat_ms=300,
                                                        timeout=10)
    if previous_tuning_log:
        print(f"Tuning using existing tuning results: {previous_tuning_log}")
        tuner = auto_scheduler.TaskScheduler(tasks=tasks,
                                             task_weights=task_weights,
                                             load_log_file=previous_tuning_log)
    else:
        tuner = auto_scheduler.TaskScheduler(tasks=tasks,
                                             task_weights=task_weights)
    print(
        f"#tasks = {len(tasks)}; ideal number of trials = {len(tasks) * 800}")
    num_measure_trials: int = configs.num_measure_trials
    if configs.continuous_tuning:  # If we are using continuous tuning
        num_measure_trials = configs.num_measure_trials - configs.ref_num_measure_trials

    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=num_measure_trials,
        verbose=1,  #Default
        early_stopping=configs.early_stopping,  # Default
        num_measures_per_round=64,  # Default
        runner=measure_ctx.runner,
        measure_callbacks=[auto_scheduler.RecordToFile(tuning_record_path)])
    tuner.tune(tune_option=tune_option)


def main():
    from time import perf_counter
    from os import environ

    environ["TVM_NUM_THREADS"] = "16"  # Use all 8 threads available

    for name, (layout, dtype, shape) in configs.models.items():
        print("Getting models...")
        model: tvm.IRModule
        model, params, input_shape, output_shape = get_relay_network(
            name=name,
            batch_size=configs.batch_size,
            layout=layout,
            dtype=dtype,
            workload_shape=shape)
        print("Get models OK")

        tasks, task_weights = auto_scheduler.extract_tasks(
            model["main"], params, configs.target)

        # Prints out each tuning task in stdout at the beginning the tuning process.
        for idx, task in enumerate(tasks):
            print("========== Task %d  (workload key: %s) ==========" %
                  (idx, task.workload_key))
            print(task.compute_dag)

        # Tuning log naming format: <model_name>-<data layout>-<data type>-<batch_size>_<trials>
        tuning_record_name = f"{name}-{layout}-{dtype}-{configs.batch_size}_{configs.compiled_suffix}_{int(time())}"
        tuning_record_path = configs.tuning_logs_path.joinpath(
            tuning_record_name)
        tuning_record_copy = configs.tuning_logs_path.joinpath(f"{name}.log")
        previous_tuning_log: str | None = None  # None as default
        if configs.continuous_tuning:  # If continuous tuning is enabled, we will select
            # targets/<target>/trials_<num_measure_trials>_batchsize_<batchsize>/<model_name>.log
            # to use as reference by default
            ref_tuning_record = configs.ref_tuning_logs_path.joinpath(
                f"{name}.log")
            previous_tuning_log = str(ref_tuning_record)

        print(f"Saving tuning log to {str(tuning_record_path)}")
        start = perf_counter()
        auto_scheduler_tuning(tasks=tasks,
                              task_weights=task_weights,
                              tuning_record_path=str(tuning_record_path),
                              previous_tuning_log=previous_tuning_log)
        end = perf_counter()
        print(f"Tuning took {end - start} seconds to complete")

        from shutil import copyfile
        if configs.continuous_tuning:
            # If we enabled continuous tuning, we need to include
            # the reference tuning log we used as the first part of
            # our current tuning logs
            from subprocess import run

            def shell(s: str):
                run(s, shell=True)

            shell(f"cat {previous_tuning_log} >> /tmp/newfile")
            shell(f"cat {str(tuning_record_path)} >> /tmp/newfile")
            shell(f"mv /tmp/newfile {str(tuning_record_copy)}")
        else:
            # make a copy with standardized name, old logs have
            # existing copies with timestamped names
            copyfile(tuning_record_path, tuning_record_copy)


if __name__ == "__main__":
    main()
