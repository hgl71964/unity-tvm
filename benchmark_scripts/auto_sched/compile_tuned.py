"""
Generates optimised tensor code for model, using relay model
and Ansor tuning logs
"""

# System utilities
from typing import List, Dict, Optional
from pathlib import Path

# Various TVM modules used
import tvm
from tvm import relay, auto_scheduler  # relay: Loading relay modules
# auto_scheduler: compiling Ansor optimised code

# Numpy, used for typing only
from numpy import ndarray

# Source files in local directory
import configs
from get_network import get_relay_network

target = configs.target


def compile_with_tuning_records(mod: tvm.IRModule,
                                params: Dict[str, ndarray],
                                log_files: str | List[str] | Path | List[Path],
                                n_lines: Optional[int] = None):
    """
    Compile a Relay function that can run on TVM graph executor, using existing Ansor tuning
    results stored in tuning-logs.

    Parameters
    ----------
    mod: tvm.IRModule
        The IRModule that holds the definition of the model. This can be generated by
        the :func:`~get_relay_network~` function.
    params: Dict[str, ndarray]
        The model parameters. This can be generated by the :func:`~get_relay_network~`
        function, or any function provided by tvm that converts other model formats to
        relay or tensorIR.
    log_files: str | List[str] | Path | List[Path]
        The tuning logs generated by Ansor. This can be a single tuning log, a list of
        logs (in which case the best schedule for each operator from all the files will
        be selected).
    n_lines: Optional[int]
        An optional parameter. When supplied, this will select the first n lines of the
        tuning log. Since all the entries in the tuning log are generated in
        chronological order, this selects the best schedule generated in the first
        n trials in the tuning process. This option is useful for ploting the
        improvement over time/over trials graph.

    Returns
    -------
    factory_module: tvm.relay.backend.executor_factory.ExecutorFactoryModule
            The runtime factory for the TVM graph executor. This contains the
            optimised representation for each tensor operator, that can either
            be runed directly in the TVM graph executor, or be stored in a
            file and loaded for future usage.
    """
    with auto_scheduler.ApplyHistoryBest(log_files, n_lines=n_lines):
        with tvm.transform.PassContext(
                opt_level=3, config={"relay.backend.use_auto_scheduler":
                                     True}):
            factory_module = relay.build(mod, target=target, params=params)
    return factory_module


def main():
    """
    This scripts respects all the parameters configured in configs.py, takes in
    the tuning logs generated from generate_tuning_logs.py, and compiles the
    optimised operator library for each tensor operator in the original model.
    This scripts
    """

    # Path names and name suffixes loaded from config
    compiled_suffix = configs.compiled_suffix
    batch_size = configs.batch_size
    tuning_log_path = configs.tuning_logs_path
    operator_libs_path = configs.operator_libs_path

    # For each model listed in config, load the relay representation of the
    # model, compiles to tvm relay factory module, and store the results into
    # individual files, whose file names are specified by the model name and
    # the suffixes defined above.
    for model_name, (layout, dtype, shape) in configs.models.items():
        # log files generated by ansor in generate_tuning_logs.py
        log_files = [str(tuning_log_path.joinpath(f"{model_name}.log"))]

        # Using get_relay_network defined in get_network.py to get model's relay
        # representation and model parameters
        print(f"Getting model parameters: {model_name}")
        mod, params, input_shape, output_shape = get_relay_network(
            batch_size=batch_size,
            name=model_name,
            layout=layout,
            dtype=dtype,
            workload_shape=shape)

        # Compile the optimised tensor code for each tensor operator
        print(f"Compiling {model_name} using tuning logs {log_files}...")
        lib = compile_with_tuning_records(mod=mod,
                                          params=params,
                                          log_files=log_files)

        # Export the library for future use
        compiled_model = f"{model_name}_{compiled_suffix}.so"
        lib.export_library(
            str(operator_libs_path.joinpath(f"{compiled_model}")))
        print(f"Exported compiled library to {compiled_model}")


if __name__ == "__main__":
    main()
